%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%% ICML 2016 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%% Use the following line _only_ if you're still using LaTeX 2.09.
%%\documentstyle[icml2016,epsf,natbib]{article}
%% If you rely on Latex2e packages, like most moden people use this:
%\documentclass{article}
%
%% use Times
%\usepackage{times}
%% For figures
%\usepackage{graphicx} % more modern
%%\usepackage{epsfig} % less modern
%\usepackage{subfigure}
%
%% For citations
%\usepackage{natbib}
%
%% For algorithms
%\usepackage{algorithm}
%\usepackage{algorithmic}
%
%% As of 2011, we use the hyperref package to produce hyperlinks in the
%% resulting PDF.  If this breaks your system, please commend out the
%% following usepackage line and replace \usepackage{icml2016} with
%% \usepackage[nohyperref]{icml2016} above.
%\usepackage{hyperref}
%
%% Packages hyperref and algorithmic misbehave sometimes.  We can fix
%% this with the following command.
%\newcommand{\theHalgorithm}{\arabic{algorithm}}
%
%% Employ the following version of the ``usepackage'' statement for
%% submitting the draft version of the paper for review.  This will set
%% the note in the first column to ``Under review.  Do not distribute.''
%\usepackage{icml2016}
%%
%%
%\usepackage{subfigure}
%\usepackage{epstopdf}
%\usepackage{wrapfig}
%
%% \usepackage{caption}
%% \usepackage[skip=0cm,list=true,labelfont=it]{subcaption}
%\usepackage{framed}
%\usepackage{color}
%\usepackage{amssymb}
%\usepackage{amsmath}
%\usepackage{amsthm}
%\usepackage{graphicx}
%\usepackage{multirow}
%% \usepackage{mathtools}
%\usepackage{url}
%\usepackage{booktabs}
%\usepackage{verbatim}
%% \usepackage[numbers,sort&compress]{natbib}
%\usepackage{booktabs}
%\usepackage{wrapfig}
%\usepackage{epstopdf}
%% \usepackage{subcaption}
%\usepackage{cleveref}
%\usepackage{verbatim}
%\usepackage{xcolor}
%% Employ this version of the ``usepackage'' statement after the paper has
%% been accepted, when creating the final version.  This will set the
%% note in the first column to ``Proceedings of the...''
%%\usepackage[accepted]{icml2016}
%
%\newcommand{\plmi}{$\pm\;$}
%
%
%\newtheorem{conj}{Conjecture}
%\newtheorem{lemma}{Lemma}
%\newtheorem{cor}{Corollary}
%\newtheorem{theorem}{Theorem}
%\newtheorem{corollary}{Corollary}
%\newtheorem{definition}{Definition}
%\newtheorem{example}{Example}
%\newtheorem{question}{Question}
%\newtheorem{assumption}{Assumption}
%\newtheorem{remark}{Remark}
%
%
%
%
%
%
%\newcommand{\nn}{\nonumber}
%\newcommand{\mc}{\mathcal}
%\newcommand{\mb}{\mathbf}
%\newcommand{\mbb}{\mathbb}
%\newcommand{\mr}{\mathring}
%\newcommand{\bd}{\bar{\delta}}
%\newcommand{\tc}{\textsc}
%\newcommand{\ceqref}{Eq.~\eqref}
%\newcommand{\ceqrefs}{Eqs.~\eqref}
%\newcommand{\mcite}{\citep}
%\newcommand{\mmcite}{\citet}
%
%
%\newcommand{\para}{\textbf}
%\newcommand{\parae}{\emph}
%
%
%\renewcommand{\algorithmicrequire}{\textbf{Input:}}
%\renewcommand{\algorithmicensure}{\textbf{Output:}}
%
%\def\prob{\hbox{Pr}}
%\def\Min{\text{Min}}
%\def\Max{\text{Max}}
%\def\bA{{\bf A}}
%\def\bB{{\bf B}}
%\def\bC{{\bf C}}
%\def\bM{{\bf M}}
%\def\bW{{\bf W}}
%\def\bP{{\bf P}}
%\def\RR{\mathbb R}
%
%\def\mg{{\mathcal G}}
%\def\bm{{\bf m}}
%\def\br{{\bf r}}
%
%\def\EE{{\mathbb{E}}}
%\def\I{{\mathbb{I}}}
%\def\bX{{\bf X}}
%\def\bx{{\bf x}}
%\def\bz{{\bf z}}
%\def\bv{{\bf v}}
%\def\bb{{\bf b}}
%\def\bt{{\bf t}}
%\def\bk{{\bf k}}
%\def\bn{\bar{n}}
%\def\bN{\bf{N}}
%\def\bd{\bar{d}}
%\def\g{\tc{g}}
%\def\G{\tc{g}}
%\def\Q{\tc{q}}
%\def\B{\tc{b}}
%\def\P{\tc{p}}
%\def\bP{{\bf P}}
%\def\H{\tc{h}}
%\def\X{\tc{x}}
%\def\A{\tc{a}}
%\def\L{\tc{l}}
%\def\Y{\tc{y}}
%\def\C{\tc{c}}
%\def\M{\tc{m}}
%\def\N{\tc{n}}
%\def\J{\tc{j}}
%\def\R{\tc{r}}
%\def\E{\tc{e}}
%\def\K{\tc{k}}
%\def\l2svd{{\ell^2_2\text{TSVD}}}
%%\DeclareMathOperator{\conv}{conv}
%%\DeclareMathOperator{\argmax}{argmax}
%%\DeclareMathOperator{\argmin}{argmin}
%%\DeclareMathOperator{\cone}{cone}
%\DeclareMathOperator{\rank}{rank}
%%\DeclareMathOperator{\var}{Var}
%%\DeclareMathOperator{\cov}{Cov}
%%\DeclareMathOperator{\col}{col}
%%\DeclareMathOperator{\diag}{diag}
%%\DeclareMathOperator{\tr}{tr}
%
%\providecommand{\norm}[1]{\lVert#1\rVert}
%
%\def\BAD{\hbox{BAD}}
%\def\for{\hbox{ for }}
%
%\newtheorem{claim}{Claim}[section]
%
%\newcommand{\details}[1]{{\color{blue}\ #1 }} %%This is the text that needs to be revised
%\newcommand{\nnote}[1]{{\color{red}\ [Navin: #1 ]}}
%
%% The \icmltitle you define below is probably too long as a header.
%% Therefore, a short form for the running title is supplied here:
%\icmltitlerunning{Robust NMF: Going Beyond Separability (Supplementary)}
%
%\begin{document}

\section{Introduction}
Let $A$ be a ~$d \times n$~ matrix (where each column, $A_{\cdot ,j}$, is a $d$ dimensional data-point)
with non-negative real entries.
\emph{Exact NMF} is the problem of factoring $A$
 into the
product $BC$ of two non-negative matrices, with $k$ columns in $B$. $k$ is generally small.
So, NMF would find the small number of ``basis vectors'' (columns of $B$) with each data point a
non-negative combination of them. This has led to the applicability of NMF \cite{Gillis}.

%%Here, we assume that $k = \mbox{rank}(A)$.
%The matrix $B$ can  reveal \emph{intrinsic} structure of the data and this
%have led to wide applicability of NMF; see \cite{Gillis} for an up-to-date review of the applications and algorithms for NMF.
%(made later in model notation) Without loss of generality we willassume that each column of $B$ sums to 1 and each column of
%$C,A$ sum to at most 1.

There has been recent interest in developing 
polynomial time bounded algorithms with proven error bounds
under specialized assumptions on the data \cite{arora2012computing,gillis2014robust,recht2012factoring,GeZ15}. 
All such algorithms require \emph{separability} assumption, first
introduced in ~\cite{DonohoStodden}.
An NMF $BC$
is \emph{separable} if after a permutation of rows of $A$ and $B$, the top $k$ rows of $B$ form a
non-singular diagonal matrix $D_0$. Using separability ~\cite{DonohoStodden}
showed that $B$ is essentially {\it identifiable} (i.e., unique) given $A$.
~\cite{arora2012computing} made a powerful observation that if
$A$ has an exact separable NMF,
and its rows are in general position, the rows of $A$ corresponding to $D_0$ can be  identified by solving
$d$ Linear Programs and then $B$ can be found.
~\cite{recht2012factoring} gave a clever reduction to a single linear program in $n^2$ variables.
~\cite{gillis2014robust} made important theoretical and practical improvements to the Linear Program.
\cite{GeZ15} introduces \emph{subset separability}, a milder assumption
than \emph{separability}, but requires solving many convex programs.  Algorithms
based on Linear and Convex programs
are hard to scale ~\cite{Gillis} because of their high complexity.
Besides separability, all algorithms make the stronger assumption of reasonably large diagonal entries
in $D_0$.

In practice, $A$ is not exactly factorable into $BC$, but $A=BC+N$, where, $N$ is the noise matrix.
(As before, $B,C$ have non-negative entries, but $N,A$ can have negative entries.)
%
An algorithm for the \emph{Noisy NMF} problem seeks to compute a $d\times k$ matrix
$\tilde B$ \footnote{For a matrix
$M$, $||M||_1$ denotes the maximum $l_1$ norm of a column of $M$. In the introduction, we use $\varepsilon$ to denote a generic small quantity
and each instance of it may have different values.}
%
for which
\begin{equation}\label{result-error}
||B-\tilde B||_1\leq\varepsilon.
\end{equation}
where $B$ is the true matrix.
[Once $\tilde B$ is found, all algorithms find a $\tilde C$ by
minimizing $||A-\tilde B\tilde C||_1$.]
Existing separability-based algorithms rapidly deteriorate in the presence of noise.
They first identify a set $K$ of
$k$ special rows and columns of
the data matrix $A$ (which correspond to $D_0$) and use
only $K$ to find $\tilde B$. Noise in any one of the $n$
data columns can change the choice of $K$
and thus the answer. So,
they all have to require that
the $l_1$ norm of EACH column $N$ be much smaller than the
$l_1$ norm of the column of $BC$.
%Assuming without loss of generality that each column of $B$ sums to 1
%and each column of $C$ sums to at most 1, so each column of $BC$ sums to at most 1, [See Supplement
%for the simple argument.], they require $\| N_{.,j}\|_1 \le \varepsilon$.
Substantial noise in a SINGLE data-point can destroy the model hypothesis.
But, in important applications like Topic Modeling, for almost all $j$,
$||N_{\cdot, j}||_1\approx ||(BC)_{\cdot ,j}||_1$ !

In this paper we introduce the following noise model, which we will
refer to as \noise model in the sequel.
\begin{equation}\label{eq:noise}
\forall T\subseteq [n]\; ,\; \mbox{ with }|T|\geq \varepsilon n\; ,\; \frac{1}{|T|}\left|\left| \sum_{j\in T} N_{\cdot, j} \right|\right|_1\leq\varepsilon.
\end{equation}
We assume without loss of generality that each column of $B$ sums to 1 and each column of $C$ sums to at most 1. (See Remark \ref{normalize}).
We require that the noise be small only when averaged over
$\Omega(\varepsilon n)$ columns of $N$ (instead of for each column).

We study \emph{Noisy NMF} under the \noise condition. More formally
we pose the following problem.
%If $N$ be i.i.d. ${\cal N}(0,\sigma^2)$ random variables one
%can show that, by borrowing tools from Random Matrix Theory(see Section \ref{sec:gaussiid}),
%when $\sigma \in O(1/\sqrt d)$, almost for every $j$
%$||N_{\cdot, j}||_1\in O(\sqrt d)$, whenever $||A_{\cdot, j}||_1\leq 1$.

{\bf Problem Definition:} If $A$ satisfies $A = BC + N$
find $\tilde{B}$ which can satisfy \eqref{result-error}, under \noise \eqref{eq:noise}.

{\bf Contributions:} As mentioned earlier existing separability based algorithms will not be able to solve the problem of interest. 
We show that under
slightly altered assumptions it is possible to recover the correct NMF efficiently under heavy noise.
\begin{itemize}
\item  {\bf Heavy Noise Model:}
Since the noise condition (\ref{eq:noise}) only bounds noise in an average of many columns,
it can hold even when the condition $||N_{\cdot, j}||_1\leq\varepsilon$ (imposed by earlier papers)
is violated for almost all $j$. 
Indeed, we prove that our noise model (\ref{eq:noise})
subsumes
several models of theoretical and practical interest,
including spherical Gaussian noise (with the amplitude of the noise
much greater than the data), general Gaussian noise and multinomial noise (arising
in Topic Modeling)
among others.
The proof of these
is based on recent results bounding the
eigenvalues of random matrices (with independent vector-valued random
variables as columns) and some
standard tools from Probability - concentration inequalities (H\"offding-Azuma).
The word ``heavy'' refers to the fact that noise can be as large as or even larger than data
in individual columns.

\item{{\bf Dominant NMF} and \name~}
Like previous papers, we make assumptions on $B$ as well as $C$.
On $B$, our assumptions weaken separability in two ways. Instead of a diagonal matrix $D_0$, we
require only a diagonally dominant matrix - the off-diagonal entries are to be smaller, not necessarily 0.
Also instead of a single row $i(l)$ for each column $l$ of $B$ with $B_{i(l),l}\not= 0$, we allow a
set $S_l$ of rows. In the context of Topic Modeling, this would be multiple ``anchor words'',
an issue which has been an open question in earlier papers. This is the Dominant Feature Assumption (D1)
spelt out later.

We make two assumptions on $C$. Each column must have a dominant entry. We call this
the Dominant Basis Vector Assumption (D2). Furthermore, there must be a (small) fraction of ``nearly pure
records'', namely, for each $l=1,2,\ldots k$, there are at least $\varepsilon_0n$ columns of $C$ with
$C_{lj}\geq 1-\varepsilon$, see Assumption D3 later. 
If a matrix pair $B,C$~satisfies  assumptions D1-3 then it is said to 
be  Dominant NMF family. 
%This condition is essential in general for identifiability of Noisy NMF.

Dominant NMFs can be recovered in presence of heavy noise by \name.
\name is based on three crucial steps: (i) Thresholding, (ii) Clustering based on SVD, and (iii) identifying dominant
basis vectors and then Dominant Features and then nearly pure records. 
Our main contribution is to show that \name recovers the correct NMF 
if data satisfies the Dominant NMF assumption.
The overall complexity is
$O((n+d)^2 k)$ (compared to the $O(n^3d+n^{2.5}d^{1.5})$ of best previous algorithm).
Both Dominant NMF and \name are inspired from \cite{tsvd} but have crucial differences. 

%\subsection{Computational Complexity}
%
%The most time-consuming step of our algorithm is a truncated Singular Value Decomposition
%(finding the top $k$ singular vectors of a $n\times d$ matrix $D$ constructed by the
%algorithm), with the special condition (proved in Lemma (\ref{s-value-gap}) in the Supplement)
%that there is a substantial gap between the $k$ th and $k+1$ st singular values.
%The the overall complexity is
%$O((n+d)^2 k)$. Moreover this algorithm is numerically stable. In comparison
%the best previous polynomial time algorithm of ~\cite{gillis2014robust} solves a Linear
%Program in $O(n^2)$ variables with a constraint matrix having $O(n^2d)$ non-zero entries.
%The best complexity algorithm for LP is from a recent result of \cite{lee-sidford} ??? - in this
%context it is:
%. Indeed empirically (as has already been observed by
%\cite{gillis2014robust}, we find that this and other provable algorithms
%do not scale up well. However, there are other heuristics which do run fast.

%\subsection{Our Model}
%
%{\bf Dominant NMF model:} We introduce the {\bf Dominant NMF} model which
%consists of {\bf Dominant Features} and {\bf Dominant Basis} assumptions.
%The  {\bf Dominant Features} assumption is on $B$ matrix and is much weaker than Separability. Instead of requiring that $B$ contain a diagonal matrix
%(after suitable row permutation) with large entries,
%we require a block-diagonally-dominant matrix, where, the off-diagonal entries
%are required to be smaller (not 0) than the diagonal ones; also we do not require individual entries to be as large.
%
%We introduce two assumptions on $C$ : the {\bf Dominant Basis}
%assumption, which has no precedence in existing NMF models, and the {\bf Pure Records} assumption,
%which is basically a version of the separability assumption, but now on $C$ instead of $B$.
%
%??????????? NECESSARY FOR IDENTIFIABILITY UNDER NOISE --- PROVE ???????????????

\item{\bf Identifiability}
One subtle but important remark about our algorithmic contribution above is that it seems to suggest the
uniqueness of the factorization $B$ because of \eqref{result-error}. This, however, is only implied for
$B, C$ satisfying the Dominant NMF model, and does not follow for unrestricted NMFs. We prove that in fact
such a uniqueness result holds even for the unrestricted NMF. This is clearly an important and desirable
characteristic of the problem making it well-posed. 

\cite{DonohoStodden} first argued the importance of Identifiability (uniqueness of $B$),
introduced the notion of separability and showed that it implies identifiability. Indeed,
under separability,
since, every row of $C$ has a scaled copy of it in $A$ (and if the rows of $A$ are in general position), then,
these rows are the extreme rays of the cone spanned by all the rows. So, by Linear Programming, we can identify them.
With our noise model, since single columns/rows can be extremely corrupted, this argument does not work anymore.
We supply the first proof of (approximate) identifiability (Theorem \ref{thm:approx-nmf}) under a set of assumptions on $B,C$ similar to the ones
we use for computation.
\end{itemize}
\begin{comment}
\subsection{Emperical results}
We present empirical evidence that our algorithm outperforms previous algorithms with
proven polynomial time bounds and error bounds. We also present empirical evidence
that our assumptions on $B,C$ are satisfied.

**********************

POSTPONE EXPERMIENTAL RESULTS

***********************

\subsection{Experimental Results}

We demostrate experimentally hat our algorithm can tolerate much more noise than previous provable algorithms \cite{gillis2014enhancing,mizutani2014ellipsoidal,spa} (2) Indeed,
experimentally, we demonstrate this. When separability does not hold but dominant features
are present, UTSVD comprehensively outperforms existing models.

*************************

END OF POSTPONE

***************************
\end{comment}

\section{Heavy Noise subsumes several noise models}

As opposed to previous noise models, ours subsumes several noise models.

{\bf Independent Gaussian Noise}: Suppose the entries of $N$ are i.i.d. ${\cal N}(0,\sigma^2)$ random variables.
We prove the following:
\begin{lemma}\label{sph-Gaussian}
{\bf Informal Statement}
\begin{itemize}
\item If $\sigma\in O(1/\sqrt d)$, then, (\ref{eq:noise}) holds whp.
\item If $\sigma \geq \Omega(1/\sqrt d)$, the MLE of $B$
violates (\ref{result-error}) (even with $k=1$).
\item If $\sigma\geq \Omega(1/d)$, then, for MOST $j$, $||N_{\cdot, j}||_1 \geq c_5$.
\end{itemize}
\end{lemma}
Since MLE is the ``best possible'' the second part is an impossibility result - noise greater than $\Omega(1/\sqrt d)$
cannot be tolerated.
(Lemmas (\ref{Gaussian-Supp}, \ref{Gaussian-Supp-2})) and the proof in Supplement specify precisely what is inside $O,\Omega$ .
The proof of the first part draws on Random Matrix
Theory, in particular, bounds on the largest eigenvalue. The other parts are simple.

{\bf General Correlated Noise}: While noise in different data points may be independent,
noise in different coordinates of the same data point need not be. We can model this more general case
by having $N_{\cdot, j}$ be independent, not neccessarily identical, vector-valued random variables. Suppose $\Sigma_j$
is the covarience matrix of $N_{\cdot,j}$ for $j=1,2,\ldots ,n$. 
We prove (see Supplement)
:\footnote{For a matrix $M$, $||M||_2$ is the spectral norm.}
\begin{lemma}
If $\left|\left|\Sigma_j\right|\right|_2\leq O(1/\sqrt d)$~then~
(\ref{eq:noise}) holds whp. 
\end{lemma}

This is tight by the impossibility result earlier.
\paragraph{\bf Heavy Noise} In the above two cases, noise was bounded by $O(1/\sqrt d)$ in EACH direction.
Here, we will only assume $||N_{\cdot, j}||\in O(1)$ (comparable to data). But $l_2$
length won't do (as is seen from part 2 of Lemma (\ref{sph-Gaussian}).

\begin{lemma}
Suppose $||C_{\cdot, j}||_1=||A_{\cdot, j}||_1=1$ and suppose
each column $N_{\cdot, j}$ of $N$ is the average of $m\geq 8c_0^2/\varepsilon^4$ independent zero-mean vector-valued random variables
$N_{\cdot,j}^{(1)},N_{\cdot,j}^{(2)},\ldots , N_{\cdot, j}^{(m)}$-
with $||N_{\cdot, j}^{(t)}||_1\leq c_0$, $c_0$ any constant.
\footnote{Note that this allows the noise to be as large (in $l_1$ norm) as data.}
Then, (\ref{eq:noise}) is satisfied whp for large enough $n$.
\end{lemma}

The proof (Supplement) is based on the
bounded difference inequality for Martingales.
(See for example \cite{mcdiarmid}).

{\bf Multinomial Noise} (A special case of Heavy noise):
Assume $||C_{\cdot, j}||_1=||A_{\cdot, j}||_1=1$ (in addition to $||B_{\cdot, j}||_1=1$).
Also assume there is $d\times n$ matrix $P$ with non-negative entries and column sums 1.
$P$ could be just $BC$.
$N_{\cdot, j}$ is the average of $m$ i.i.d. Multinomial trials, each of which picks
 a unit vector $e_i$ with probabilities proportional to $P_{i,j}$. Subtract the mean to make $E(N_{ij})=0$. So,
 $$E(N_{ij})=0\; ;\; \var(N_{ij}) = \frac{P_{ij}}{m}.$$
[In the example of Topic Modeling, the process above has picked $m$ words in each document. Usually, $m<<n,d$]
Again, almost all data points can violate the condition $||N_{\cdot, j}||_1\leq
    \varepsilon$.

\paragraph{\bf Adversarial Noise:} All the above noise models are stochastic. A non-stochastic noise model which
is also a special case of our noise model:
\begin{lemma}
Suppose we have adversarial (not random) corruption (up to $l_1$ norm 1 each) of
at most an $\varepsilon^2$ fraction of all data points. Then, (\ref{eq:noise})
is satisfied.
\end{lemma}
For example in Image Processing, this allows for an $\varepsilon^2$ fraction of images to have large corruption which can be used to model occlusion \cite{oh2008occlusion}.

%     {%\bf Multinomial Noise} For this, assume $||C_{\cdot, j}||_1=||A_{\cdot, j}||_1=1$.
%     $N_{\cdot, j}$ is the average of $m$ i.i.d. Multinomial trials, each of which picks
% a unit vector $e_i$ with probabilities proportional to $(BC)_{i,j}$. Subtract the mean to make $E(N_{ij})=0$. So,
%    $$E(N_{ij})=0\; ;\; \var(N_{ij}) = \frac{(BC)_{ij}}{m}.$$
%    [An example of this is Topic Modeling, where the process above has picked $m$ words in each document. Usually, $m<<n,d$
%    Almost all data points can violate the condition $||N_{\cdot, j}||_1\leq
%    \varepsilon$ (Lemma (\ref{Multinomial-Supp}) in Supplement). However, we will also show (again using Random Matrix Theory) that our noise assumption is satisfied.

\section{Dominant NMF model}\label{subsec:assumptions}


We say that $A, B, C, N$ with $A = BC + N$ and $B, C$ non-negative, satisfy the dominant NMF model if they satisfy the following four conditions. 

We call the rows of $A$ (indexed by $i$) ``features'' and the columns (indexed by $j$) ``records.''
The columns (indexed by $\ell$) of $B$ are called ``basis vectors.''
%The Dominant NMF model makes three main assumptions. The first is that each basis vector has a set of {\bf Dominant Features}.
%The basis vector has high component in its dominant features and low component in the dominant features
%of other vectors. This is weaker than separability which replaces ``low'' by ``zero''. Another generalization
%of separability is that each basis vector
%can have several dominant features.
%[It is unknown how to extend LP based methods under this generalization; the question was raised in \cite{arora2012computing}.]

%In the example of Image understanding \cite{lee1999learning}, each basis vector
%may be a disjoint part (nose, mouth etc.) and in such a situation, the dominant features would just be the high-intensity pixels
%(not necessarily just one ``pure pixel'' as under separability) of that part.
%
%The second main assumption is
%that almost
%all records have {\bf Dominant Basis Vectors}, i.e., one basis vector on which the record puts strictly higher
%weight than others. In Topic Modeling, this is the primary subject of a document, and in image understanding
%it could be the main focus of the image. We now state the model and results more precisely.
%
%A third assumption called the ``Nearly Pure Records Assumption'' which hypothesizes that for each basis vector,
%there is a (small, but constant) fraction of records which are nearly purely that basis vector. We will see both
%empirical justification as well as necessity of this for identifiability under heavy noise. We need the following
%before we formally state the assumptions.

\begin{remark}\label{normalize}
We may assume without loss of generality that each column of $B$ sums to 1 and each column of
$C$ sums to at most 1.
We can divide each column of $B$ by its $l_1$ length (ensuring $||B_{\cdot, j}||_1=1$) and multiply
the corresponding row of $C$ by the same amount and preserve $BC$. Then, we can scale each
column of $C$ (and correspondingly each column of $A,N$) so as to ensure $||C||_1\leq 1$.
Note that this only scales the entire error.
\end{remark}


In the following, $0< \epsilon, p_0, \gamma, w_0,\rho ,\alpha ,\beta < 1$ etc. are constants; $\epsilon$'s should be thought
of as being small.
%Wlg assume  $\norm{B_{\cdot,j}}_1=1$ and $\norm{A_{\cdot,j}}_1\leq 1$ for all $j$.
%These imply $\norm{C_{\cdot,j}}_1\leq 1$.

{\bf D1. Dominant Features:}
There are $k$ disjoint sets of features, $S_1,S_2,\ldots ,S_k \subset [d]$,
such that (a) $\forall i\in S_l,\; \forall l'\not= l, \; B_{il'}\leq\rho B_{il}$,
(b) $\sum_{i\in S_l}B_{il}\geq p_0$, and (c) $\forall i\in S_l, B_{il}\geq \gamma$.
This assumption can be seen as an aggregate version of the separability condition: instead of asking
for an anchor word in each topic we ask for catchwords (this assumption is essentially
the catchwords assumption in \cite{tsvd}). This is also a relaxation of the assumption in
\cite{lee1999learning} who require the basis vectors to have disjoint support.

{\bf D2. Dominant Basis Vectors}
There is a partition of $[n]$ into $T_1,T_2,\ldots ,T_k$ satisfying
(a) $\forall j\in T_l,l'\not= l, C_{l,j}\geq\alpha \text{  and  }  C_{l',j}\leq\beta$, and
(b) $\forall l, |T_l| \geq w_0 n$.
%We assume $w_l\geq w_0$ for all $l$.
This is similar to the dominant topic assumption in Bansal et al.\cite{tsvd} which says that each document has
a dominant topic.

{\bf D3. Nearly Pure Records Assumption.}
For each $l$, there is a set $P_l$ of at least $\varepsilon_0 n$ records in each of which the $l$'th basis vector has
coefficient at least $1-\varepsilon_4$. I.e.,
$\forall l, \exists \geq \varepsilon_0n \; j\mbox{   with  } C_{lj}\geq 1-\varepsilon_4$.
This is similar to the previous assumption. It is true in the context of LDA (with low hyperconcentration paramter).
The previous two conditions allow for the robustness of our results.

{\bf D4. Noise} We assume $N$ satisfies the following quantitative version of \eqref{eq:noise}:
\begin{equation}\label{eq:noise-2}
\forall T\subseteq [n]\; ,\; |T|\geq \varepsilon_4n\; ,\; \frac{1}{|T|}\left|\left| \sum_{j\in T}N_{\cdot,j}\right|\right|_1\leq\varepsilon_4^2.
\end{equation}

A matrix pair $B,C$ is defined to be part of the {\bf Dominant NMF} family 
if they satisfy Assumptions {\bf D1-3}. 
\section{\name:A SVD based algorithm for NMF}
In this section we describe an algorithm which yields an NMF from the SVD 
decomposition of matrix obtained by carefully thresholding the data-matrix ~$A$.
We will also show that it provably recovers $B$ under heavy noise  if $B,C$ 
comes from a Dominant NMF family. 
\paragraph{Constants used in the Algorithm}
$\alpha,\beta,\rho,\varepsilon ,\varepsilon_0, \varepsilon_4, \gamma$ in $(0,1)$ and $\nu >1$.
All but $\nu$ have already been introduced. $\nu$ is defined in the algorithm.
The actual values of all these constants may not be needed in pracitce; see Remark (\ref{constants-needed}) in the
Supplement. We assume the constants satisfy:\\
$
\varepsilon < \varepsilon_0/20;
\beta+\rho \leq (1-5\varepsilon )\alpha;
 \varepsilon_4\leq \frac{\alpha\gamma \varepsilon}{2},\frac{\varepsilon_0 w_0 p_0}{16k^3},\frac{\varepsilon_0^2}{4}.$
%
\paragraph{\name}

{\bf Input:} $\bA$,$k,\alpha,\varepsilon,\varepsilon_4,\beta +\rho,\varepsilon_0$
{\bf Output:} Basis matrix $\bB$.
\begin{enumerate}
%     \item  Randomly partition the columns of $\bA$ into two
% matrices $\bA^{(1)}$ aqnd $\bA^{(2)}$ of $s$~columns each.
    \item {\bf Thresholding}:
	    Apply the Thresholding procedure (see below) to get $\textbf{D}$ and $k$.
    \item {\bf SVD}: Find the best rank $k$ approximation $\textbf{D}^{(k)}$ to $\textbf{D}$.
    \item {\bf Identify Dominant Basis Vectors for each record}:
    \begin{enumerate}
       \item {\bf Project and  Cluster} Find (approximately) optimal $k$-means clustering of
        the columns of $\textbf{D}^{(k)}$.
        \item {\bf Lloyd's Algorithm} Using the clustering found in Step 3(a) as the starting clustering,
            apply Lloyd's $k$-means algorithm to the columns of $\textbf{D}$ ($\textbf{D}$, not $\textbf{D}^{(k)}$).
        \item   Let $R_1,R_2,\ldots ,R_k$ be the $k-$partition of $[n]$
        corresponding to the clustering after Lloyd's.
    \end{enumerate}
    \item {\bf Identify Dominant Features for each basis vector}:
   \begin{enumerate}
        \item For each $i,l$, compute $g(i,l)=$ the $(\lfloor \varepsilon_0n/2 \rfloor)$th highest element of $\{ A_{ij} : j\in R_l\}$.
	\item $J_l=\{ i: g(i,l)>\mbox{Max}(\gamma-2\varepsilon_4, \mbox{Max}_{l'\not=l} \nu g(i,l'))\}$, where,
			$\nu=\frac{1-\alpha\varepsilon}{\beta+\rho+2\alpha\varepsilon}$.
   \end{enumerate}
   \item {\bf Find Basis Vectors}  Find the $\lfloor \varepsilon_0n /4\rfloor $ highest $\sum _{i\in J_l}A_{ij}$ among all $j\in [n]$ and return the average
   of these $A_{\cdot,j}$
   as our approximation $\hat B_{\cdot, l}$ to $B_{\cdot,l}$.
\end{enumerate}
%
%
\subsection{Thresholding Procedure}

\begin{enumerate}
	\item Initialize $R:=[d]$. /* $R$ is the set of unpruned words.*/
	\item For each $i$,
	\begin{enumerate}
	\item compute $\nu_i$ the $(1-\frac{\varepsilon_0}{2})-$fractile of row $i$ of $A$. Let $\zeta_i:=\alpha \nu_i-2\varepsilon_4$.
		($\zeta_i$ is the threshold for row $i$ of $A$.)
	\item If $\zeta_i\geq 0$, set $W_i:=\{ j: A_{ij}\geq \zeta_i \}$. Set $D_{ij}:= \sqrt{\zeta_i}$ for $j\in W_i$ and $D_{ij}:=0$
		for $j\notin W_i$.
	\item If $\zeta_i<0$, then, set $W_i:=\emptyset\, ;\, D_{ij}:=0 \, \forall j\; ;\; R := R\setminus \{i\}$.
	\end{enumerate}
	\item Sort the $|W_i|$ in ascending order. For convenience, renumber the $i$ so that now $|W_i|$ are in the ascending order.
	\item For $i=1,2,\ldots, $ in $R$: (If $W_{i}\tilde \subseteq W_{i'}$,
we ``prune'' $i'$ by zeroing out all entries not in $W_i$.)
		\begin{itemize}
			\item For $i'>i$ with $i'\in R$, and $|W_{i}|\leq |W_{i'}|-\varepsilon_0 n/8$, if $W_{i}\tilde \subseteq W_{i'}$,
		\footnote{If $W,W'\subseteq [n]$, we write $W\tilde \subseteq W'$ to denote: $|W\setminus W'|\leq\varepsilon_0 n/4$}
		set $D_{i',j}:=0$ for all $j\in W_{i'}\setminus W_{i}$; set $W_{i'}$ to $W_i$ and
		   delete $i'$ from $R$.
		  \end{itemize}
\end{enumerate}
$D$ is the $d\times n$ matrix after thresholding.

{\bf Remark} The pruning step as stated above takes $O(nd^2)$ time. This can be reduced
to $O^*(d^2)$ (assuming $\varepsilon_0/k\in \Omega(1)$) by the following: Instead of comparing
$W_i$ to $W_{i'}$, we just pick a uniform random sample $T$ of $O(k\log d/\varepsilon_0)$ $j$ 's at
the outset and only compare the sets $W_i\cap T$ and $W_{i'}\cap T$. The proof that this succeeds
with high probability is simple and we postpone that to the final paper.

{\bf Remark }{\tt by Jagdeep} Given the value of $\nu$, the constants $\rho$, $\beta$ and $\epsilon$ are not required for the algorithm.
Empirically we see that the values of $\gamma$, $\epsilon_4$ are not sensitive to the recovery of the factorization and can be fixed to any small number close to zero (e.g $10^{-4}$) .
We fixed the other constants as $\epsilon_4 = 0.04$, $\alpha = 0.9$, $\nu  = 1.05$ and used the same in all the experiments.
Note that a proper tuning to find the constants may improve the result further.

\begin{theorem}
Given a $d \times n$ data-matrix $A$ and
under the assumptions {\bf D1-4}, the algorithm \name finds for each $l$, an approximation $\hat B_{\cdot, l}$ satisfying
$$\left| B_{\cdot, l}-\hat B_{\cdot, l}\right|_1\leq \varepsilon _0 .$$
\end{theorem}
% This section will be moved to intro later.
%
%
%
The proof of the above theorem appears in the supplement.

\emph{Intuition for the algorithm.} Thresholding is a crucial part of the algorithm.
It would ideally ensure that the thresholded matrix $D$
 is
a block matrix with non-zero entries precisely in $S_l\times T_l$. But with errors, this is only approximately true.
Also, since there are many features which are not dominant for any basis vector, the effect of thresholding on them
is more difficult to control. Our careful choice of thresholds and the pruning help in ensuring block diagonality for these
features too. Then we apply SVD + k-means which clusters records according to dominant basis vector. Now taking the
average of each cluster (which is what one does normally in hard clustering)
does not work here, because here,
basis vectors are  essentially the extreme points of each cluster.
We tackle this by identifying dominant features first, then
identifying records which have the largest component in these features. We show that these records are near the extreme
points and show that their average does the job.

\paragraph{\bf Discussion}
The Dominant NMF model closely parallels the work of \cite{tsvd} on Topic Modeling, in particular Dominant Features are inspired from Catchwords while Dominant Basis Vectors are inspired from Dominant topics.
Algorithm \name differs from TSVD\cite{tsvd} in  several crucial points: First, there is no stochastic
model here of $C$, so the proof of correctness of \name is geared for the ``worst case'' which is in contrast with \cite{tsvd}. Secondly,
the \noise assumption \eqref{eq:noise-2} is new and we prove that under this
noise we recover $B$ with high probability.
Thirdly, in Topic Modeling, \cite{tsvd} used crucially a technical assumption of ``no local min''
which does not hold in general in NMF. This causes a change in the
thresholding step, and the
entire proof that thresholding does its job is now different. Finally, a major contribution of the present paper
is to prove the Identifiability of NMF under fairly general and natural assumptions; \cite{tsvd} does not consider identifiability.

\subsection{ Identifiability of Dominant NMF} \label{subsec:unique}
\begin{comment}
Given a non-negative factorization $A=BC$ we can generate many others of the form $A=B'C'$ where
$B'=BP$ and $C'=P^{-1}C$ and $P$ is a $k \times k$ matrix obtained by applying a permutation matrix to
a diagonal matrix
with nonnegative entries. This just amounts to scaling and permuting of the columns of
$B$ and correspondingly of the rows of $C$. If this accounts for all factorizations of $A$ then we
say that $A$ has unique NMF. For some applications of NMF, it is desirable to have the property that the
NMF is unique: It gives us confidence that NMF has found the ``right'' structure in the data and
not some spurious explanation.
E.g., in
clustering applications it tells us that the clusters are unique, in topic modeling it tells us that
the ``right'' topics have been found.
In general, NMF need not be unique and this raises the question of which matrices $A$ have
unique NMF.
\end{comment}
In their influential paper~\cite{DonohoStodden}, Donoho and Stodden consider the
question of uniqueness of nonnegative matrix factorization for exact NMF.
% Uniqueness of NMF suggests that NMF has found ``meaningful'' structure in the data and not an accidental one.
There has been considerable work on understanding uniqueness
conditions since then; we refer to Huang~et~al.~\cite{Huangetal} and Gillis~\cite{Gillis}
and references therein for an
up-to-date review of the literature. Some of these conditions are necessary and sufficient;
unfortunately they do not seem to be easy to check or use.
These conditions are often geometric in nature and not directly related to the application at hand.
Donoho et al. gave a set of necessary conditions called Separable Factorial Articulation Family
including separability.
Laurberg~et~al.~\cite{Laurberg} gave further such conditions and also studied the approximate
NMF case.

\begin{comment}
If $B$ is separable and there is no noise, then, clearly, there is a scaled copy of each row of $C$ in $A$.
Under the mild assumption that the rows of $A$ are in general position, all rows of $A$ are non-negative
linear combinations of these rows and these are the only rows which are not non-negative combinations of
other rows. [So, they are the extreme rays of the cone generated by them.] This essentially proves
uniqueness. Now suppose we do not assume uniqueness. Then, we will argue that uniqueness is no more
automatic. Consider the simple case when $k=2=d$ and
$$B=\left( \begin{array}{cc} q_0 &q_1\\ q_1&q_0\end{array}\right),$$
where, $q_0>q_1>0$ and $q_0+q_1=1$. Whatever $C,A$ are, we do not get uniqueness: If
$$B' = B \left(\begin{array}{cc} 1+\varepsilon & -\varepsilon \\ -\varepsilon & 1+\varepsilon \end{array}\right),$$
then, for small enough $\varepsilon$, $B'$ has non-negative entries and since the columns of $B$ are convex
combinations of the columns of $B'$, say, $B=B'W$, and if $A=BC$, then $A=B'(WC)$ is another factorization.
This illustrates that in addition to the Dominance conditions we had for the algorithm, we also need to
ensure that $q_1$ is much smaller than $q_0$.
\end{comment}

By identifiability of approximate NMF, we mean the following: Given $k$ and an $d\times n$ matrix $A$, a $d\times k$ matrix
$B$ is (approximately) \emph{identifiable} from $A$ if there exists a $C$ with $||A-BC||_1\leq\varepsilon$ and
for any  $B',C'$ ($d\times k$ and $k\times n$ respectively) with $||A-B'C'||_1\leq\varepsilon$, we have
$||B-B'||_1\leq \varepsilon'$, where, $\varepsilon'\rightarrow 0$ as $\varepsilon\rightarrow 0$.

Under the above conditions, we have the following theorem.
\begin{theorem} \label{thm:approx-nmf}
Suppose that $B, C$ are $d\times k$ and $k\times n$ matrices with non-negative
entries where each column of B sums to 1 and they satisfy Assumptions {\bf D1} and {\bf D3}.
Suppose that $B' \in \RR_+^{d \times k}$ and $C' \in \RR_+^{k \times n}$
satisfy
\begin{align}\label{eqn:condition_BC}
\norm{\sum_{j \in R_\ell}(BC)_{\cdot, j} - \sum_{j \in R_\ell}(B'C')_{\cdot, j}}_1 \leq \delta
\sum_{j\in P_l}\norm{C_{\cdot, j}}_1\forall l.
\end{align}
Let $\delta' := 2 \epsilon + 6\delta$. Also assume that the following conditions are satisfied:
(a) $(p_0 - \delta')^2 > 4 k (p_1 + \delta')$, and (b) $2 \delta' < p_0 - p_1$.
Then $B$ is close to $B'$ up to permutations and scalings of the columns. More precisely,
there exist a permutation $\pi:[k]\to [k]$ and constants $\alpha_j \in \RR$ for $j \in [k]$
such that
\begin{align*}
\norm{B_{.,j}-\alpha_j B'_{.,\pi(j)}}_1 \leq 2 \delta' + \frac{4k(p_1+\delta')}{p_0-\delta'}.
\end{align*}
\end{theorem}
\begin{proof} See Supplement. \end{proof}
\begin{comment}
To provide some intuition, we prove an exact version of the
above uniqueness theorem.

\emph{A uniqueness theorem for exact NMF.}
An NMF $A = BC$ is said to be \emph{separable} if for each $\ell \in [k]$, there is an $i \in [d]$ such
that $B_{i,\ell}$ is the unique non-zero entry in row $i$ of $B$. An NMF $A = BC$ is said to have
the \emph{pure records} property if for each $\ell \in [k]$, there is a $j$ such that $C_{\ell, j}=1$
(and so $C_{\ell', j}=0$ for all $\ell' \neq \ell$). In the context of topic modeling, this is the same as
assuming that for each topic, there is a document purely on that topic.

\begin{theorem}
If an NMF $A = BC$, where $\rank(A) = k$, has both the separability and the pure records properties,
then, the NMF is unique. I.e., if for $d \times k$ and $k \times n$ (resp.) matrices $B',C'$ with
non-negative entries, we have $A = B'C'$, then there is a diagonal matrix $D$ with positive diagonal
entries and a permutation matrix $\Pi$ and $P=D\Pi$, such that $B' = BP$ and $C' = P^{-1}C$.
\end{theorem}
Notice that in the second factorization $A = B'C'$, we did not make any assumptions apart from
non-negativity.
\begin{proof} See Supplementary \end{proof}

\begin{proof}
We will first bring $A$ in a convenient form by permuting its columns and rows.
It's clear that $\mathsf{CH}(A) \subseteq \mathsf{CH}(B)$.
\footnote{$\mathsf{CH}(A)$: Column space of $A$.}
The pure records property of factorization $A=BC$
gives that in fact
$\mathsf{CH}(A) = \mathsf{CH}(B)$ and the columns of $B$ occur as columns of $A$. Permute the columns of $A$, if
needed, so that the first $k$ columns correspond to the columns of $B$. Now, by the separability
property of the factorization, and permuting the rows of $A$ if needed, we can arrange that the
top left $k\times k$ submatrix $D$ of $A$ is a diagonal matrix with positive entries.

Now consider any other NMF $A = B'C'$. As before, we have $\mathsf{CH}(A) \subseteq \mathsf{CH}(B')$.
This, in particular, implies $\mathsf{CH}(D) = \mathsf{CH}(A|_{[k]}) \subseteq \mathsf{CH}(B'|_{[k]})$, where
$A|_{[k]}$ is the matrix consisting of the first $k$ rows of $A$, and similarly for $B'|_{[k]}$.
It is now clear that the only way $\mathsf{CH}(D) \subseteq \mathsf{CH}(B'|_{[k]})$ can hold is that $B'|_{[k]}$ is
itself a diagonal matrix with positive entries, possibly after applying a permutation---proving the
separability of NMF $B'C'$. We now permute
the columns of $B'$ so that $B'|_{[k]}$ is a diagonal matrix. Now, it is clear that the first $k$ columns
of $C'$ must also form a diagonal matrix with positive entries---proving the pure records property
of NMF $B'C'$.
\end{proof}

We now give an outline of the proof of Theorem~\ref{thm:approx-nmf}; full proof
appears in the supplement.
\begin{proof}[Outline: ]
Without loss of generality we may assume that each column of $B'$ also sums to 1.
We aggregate the columns in $C$ and $C'$ by simply replacing the set of columns in $R_\ell$ by
their
%sum for each $\ell$ (thus replacing the set of $\abs{R_\ell}$ columns by a single column). Now assume that for $l=1,2,\ldots k$,
column $l$ of $C$ (respectively of $C'$) is the sum of all columns in $R_l$ of the original $C$ (respectively $C'$).
The Pure Records condition on $C$ can now be restated as:
For $\ell \in [k]$
\begin{align}\label{1700}
C_{\ell,\ell } \geq (1-\epsilon) \norm{C_{\cdot, \ell}}_1.
\end{align}
Condition~\eqref{eqn:condition_BC} becomes: for $\ell \in [k]$
\begin{align} \label{eqn:condition_BC_var}
	\norm{(BC)_{\cdot, \ell } - (B'C')_{\cdot, \ell}}_1 \leq \delta \norm{C_{\cdot,\ell}}_1.
\end{align}
We first outline the proof for which we need one piece of notation:
We rescale columns of $C$ and $C'$
to get matrices $\bar{C}$ and $\bar{C'}$, respectively,
as follows: $\bar{C}_{.,\ell} := C_{.,\ell}/\norm{C'_{.,\ell}}_1$ and
$\bar{C'}_{.,\ell} := C'_{.,\ell}/\norm{C'_{.,\ell}}_1$
(note that in both cases we are dividing by $\norm{C'_{.,\ell}}_1$).
For the outline, let's say two matrices $P,Q$ are approximately equal, denoted $P\approx Q$ if the $l_1$ distance between
each column of $P$ and the corresponding column of $Q$ is small (to be quantified later).
The proof has the following steps which are described in supplementary.
\begin{enumerate}
	\item We first prove $$B\approx B\bar C.$$ This plus the hypothesis (\ref{eqn:condition_BC}) will imply that
		$B\approx B'\bar C'$ which says that the convex hull of the columns of $B$, denoted CH$(B)$ is contained in the
		convex hull of the columns of $B'$:
		$$\mbox{CH}(B)\subseteq \mbox{CH}(B').$$
	\item We prove that for any $d\times k$ matrix $P$ with non-negative entries and all column sums $1$,
		with CH$(B)\subseteq $CH$(P)$, we must have
		$P\approx B$ after possibly permuting columns of $P$.
\end{enumerate}
\end{proof}g~et~al.~\cite{Huangetal}. However, these conditions do not seem
to have wide applicability.

%
\end{comment}


