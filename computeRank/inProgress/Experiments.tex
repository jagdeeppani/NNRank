\section{Experiments}

\subsection{Synthetic experiments}
In this section our goal is to show that the proposed algorithm discovers the correct non-negative rank (up to approximation) when the underlying dominant assumptions hold on the dataset. We present results under two noise models, Gaussian and Multinomial noise models. The dataset is assumed to be generated under dominant assumption.

\textit{Dominant data:} 
%Data is generated under the dominant basis vectors and dominant features assumptions.
Each column of $C$ is generated from a symmetric Dirichlet distribution with hyper-parameter $\frac{1}{2k}$.
%$\footnote{nearly $77\%$ of the data points satisfy dominant basis vector assumption ($\alpha>0.4, \beta<0.3$)}.
Columns of $B$ are also generated from Dirichlet by randomly selecting $c$ features and putting weight $\propto \eta_0$ on these features.
Concretely, let $c$ be the number of dominant features for each basis vector and $\eta_{0}$ be the sum of the weights of the dominant features in each basis vector.
Assume $\eta = (\frac{\eta_{0}}{1-\eta_{0}})\cdot(\frac{d-c}{c})$.
For the $j$th basis vector, we set $\widetilde{\alpha}_{j} = \textbf{1}_{d \times 1}$ and multiply the elements of $\widetilde{\alpha}_{j}$ indexed from $(c(j-1)+1) \hspace{1mm}\text{to}\hspace{1mm} cj$ by $\eta$.
% Vector $B_{j}$, 
$j$th column of $B$, is then generated from a Dirichlet with hyper-parameter $\widetilde{\alpha}_{j}$. We chose $c = 3, \eta_0=0.1.$

\textit{Gaussian Noise: } 
Noise matrix $N$ is generated with each entry from $\mathcal{N}(0,\sigma_{i})$ where $\sigma_{i}=\beta \times B_{il}$. $B_{il}$ is the maximum element in $i$th row of $B$.
This is based on the subset noise. We chose $\beta \in [0 , 0.5]$ as the noise level.

\textit{Multinomial Noise: }
%$N_{.j}$, $j$th column of noise matrix $N$, is generated by sampling an $i \in [1\cdots d]$ with prob (picking $i$) $= (BC)_{ij}$ $m$ times and taking the average to find $\tilde{N}_j$. Then $N_j$ is set as $\tilde{N}_j-(BC)_j$ to set the mean of noise to zero. 
The matrices $B$ and $C$ are normalized such that each of their column sums to one.
$N_{.j}$, $j$th column of noise matrix $N$, is generated by picking $m$ samples (a sample is a 1-of-d coding) from $ [1\cdots d]$ with prob($i$) $= (BC)_{ij}$ and taking an average of the samples to find $\tilde{N}_{.j}$. Then $N_{.j}$ is set as $\tilde{N}_{.j}-(BC)_{.j}$.
%to set the mean of noise $N_{.j}$ to zero. 
Lower $m$ implies high noise and vice versa, $m \in \{10,60,100\}$.
%{\bf Benchmarks:} We compare  


\textit{Parameters: } $edgeThreshold = 3 \times \frac{n}{s}$.  $\gamma = 0.5$. For higher noise level, the number of groups $s$ varies as per the table.
In the experiments we consider $d=1000, n=2000, k=20$.
Table ~\ref{table:Gaussian} presents the quality of non-negative rank discovered under Gaussian noise assumption. The rank presented is the median over 20 different datasets generated from the same underlying parameter.

Table ~\ref{table:Multinomial} presents the non-negative ranks discovered by the algorithm under multinomial assumption. 

\begin{table}[]
\centering
\caption{Non-negative rank discovered by proposed algorithm under different levels of Gaussian noise. (Is it really non-negative rank, how can we be sure?)}
\label{table:Gaussian}
\begin{tabular}{|c|c|c|}
\hline
\textbf{\begin{tabular}[c]{@{}c@{}}$\beta$\\ (Noise level)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}$s$ \\ (Number of groups)\end{tabular}} & \textbf{Discovered rank} \\ \hline
0                                                                        & 200                                                                        & 20                       \\ \hline
0.1                                                                      & 150                                                                        & 20                       \\ \hline
0.2                                                                      & 125                                                                        & 19                       \\ \hline
0.3                                                                      & 95                                                                         & 21                       \\ \hline
0.4                                                                      & 93                                                                         & 22                       \\ \hline
0.5                                                                      & 120                                                                        & 21                       \\ \hline
\end{tabular}
\end{table}


\begin{table}[]
\centering
\caption{Identifying non-negative rank under Multinomial noise. Smaller $m$ implies higher noise.}
\label{table:Multinomial}
\begin{tabular}{|c|c|c|}
\hline
\textbf{\begin{tabular}[c]{@{}c@{}}$m$ \\ (sample size)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}$s$ \\ (Number of groups)\end{tabular}} & \textbf{Discovered rank} \\ \hline
1000                                                                  & 140                                                                        & 21                       \\ \hline
500                                                                   & 340                                                                        & 19                       \\ \hline
200                                                                   & 1000                                                                       & 33                       \\ \hline
\end{tabular}
\end{table}

\subsection{Experiments on real data}














